{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Корректность проверена на Python 3.6:**\n",
    "+ pandas 0.23.4\n",
    "+ numpy 1.15.4\n",
    "+ matplotlib 3.0.2\n",
    "+ sklearn 0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод главных компонент\n",
    "\n",
    "В данном задании вам будет предложено ознакомиться с подходом, который переоткрывался в самых разных областях, имеет множество разных интерпретаций, а также несколько интересных обобщений: методом главных компонент (principal component analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming assignment\n",
    "\n",
    "Задание разбито на две части: \n",
    "- работа с модельными данными,\n",
    "- работа с реальными данными.\n",
    "\n",
    "В конце каждого пункта от вас требуется получить ответ и загрузить в соответствующую форму в виде набора текстовых файлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теория\n",
    "\n",
    "Любой набор данных представляет собой матрицу $X$.\n",
    "\n",
    "Метод главных компонент последовательно находит следующие линейные комбинации признаков (компоненты) из $X$:\n",
    "- каждая компонента ортогональна всем остальным и нормированна: $<w_i, w_j> = 0, \\quad ||w_i||=1$,\n",
    "- каждая компонента описывает максимально возможную дисперсию данных (с учётом предыдущего ограничения).\n",
    "\n",
    "Предположения, в рамках которых данный подход будет работать хорошо:\n",
    "- линейность компонент: мы предполагаем, что данные можно анализировать линейными методами,\n",
    "- большие дисперсии важны: предполагается, что наиболее важны те направления в данных, вдоль которых они имеют наибольшую дисперсию,\n",
    "- все компоненты ортогональны: это предположение позволяет проводить анализ главных компонент при помощи техник линейной алгебры (например, сингулярное разложение матрицы $X$ или спектральное разложение матрицы $X^TX$).\n",
    "\n",
    "Как это выглядит математически?\n",
    "\n",
    "Обозначим следующим образом выборочную матрицу ковариации данных: $\\hat{C} \\propto Q = X^TX$. ($Q$ отличается от $\\hat{C}$ нормировкой на число объектов).\n",
    "\n",
    "Сингулярное разложение матрицы $Q$ выглядит следующим образом:\n",
    "\n",
    "$$Q = X^TX = W \\Lambda W^T$$\n",
    "\n",
    "Можно строго показать, что столбцы матрицы $W$ являются главными компонентами матрицы $X$, т.е. комбинациями признаков, удовлетворяющих двум условиям, указанным в начале. При этом дисперсия данных вдоль направления, заданного каждой компонентой, равна соответствующему значению диагональной матрицы $\\Lambda$.\n",
    "\n",
    "Как же на основании этого преобразования производить уменьшение размерности? Мы можем отранжировать компоненты, используя значения дисперсий данных вдоль них.\n",
    "\n",
    "Сделаем это: $\\lambda_{(1)} > \\lambda_{(2)} > \\dots > \\lambda_{(D)}$.\n",
    "\n",
    "Тогда, если мы выберем компоненты, соответствующие первым $d$ дисперсиям из этого списка, мы получим набор из $d$ новых признаков, которые наилучшим образом описывают дисперсию изначального набора данных среди всех других возможных линейных комбинаций исходных признаков матрицы $X$. \n",
    "- Если $d=D$, то мы вообще не теряем никакой информации.\n",
    "- Если $d<D$, то мы теряем информацию, которая, при справедливости указанных выше предположений, будет пропорциональна сумме дисперсий отброшенных компонент.\n",
    "\n",
    "Получается, что метод главных компонент позволяет нам ранжировать полученные компоненты по \"значимости\", а также запустить процесс их отбора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример\n",
    "\n",
    "Рассмотрим набор данных, который сэмплирован из многомерного нормального распределения с матрицей ковариации $C = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "mu = np.zeros(2)\n",
    "C = np.array([[3,1],[1,2]])\n",
    "\n",
    "data = np.random.multivariate_normal(mu, C, size=50)\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Путём диагонализации истинной матрицы ковариаций $C$, мы можем найти преобразование исходного набора данных, компоненты которого наилучшим образом будут описывать дисперсию, с учётом их ортогональности друг другу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v, W_true = np.linalg.eig(C)\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "# построим истинные компоненты, вдоль которых максимальна дисперсия данных\n",
    "plt.plot(data[:,0], (W_true[0,0]/W_true[0,1])*data[:,0], color=\"g\")\n",
    "plt.plot(data[:,0], (W_true[1,0]/W_true[1,1])*data[:,0], color=\"g\")\n",
    "g_patch = mpatches.Patch(color='g', label='True components')\n",
    "plt.legend(handles=[g_patch])\n",
    "plt.axis('equal')\n",
    "limits = [np.minimum(np.amin(data[:,0]), np.amin(data[:,1])),\n",
    "          np.maximum(np.amax(data[:,0]), np.amax(data[:,1]))]\n",
    "plt.xlim(limits[0],limits[1])\n",
    "plt.ylim(limits[0],limits[1])\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь сравним эти направления с направлениями, которые выбирает метод главных компонент:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_principal_components(data, model, scatter=True, legend=True):\n",
    "    W_pca = model.components_\n",
    "    if scatter:\n",
    "        plt.scatter(data[:,0], data[:,1])\n",
    "    plt.plot(data[:,0], -(W_pca[0,0]/W_pca[0,1])*data[:,0], color=\"c\")\n",
    "    plt.plot(data[:,0], -(W_pca[1,0]/W_pca[1,1])*data[:,0], color=\"c\")\n",
    "    if legend:\n",
    "        c_patch = mpatches.Patch(color='c', label='Principal components')\n",
    "        plt.legend(handles=[c_patch], loc='lower right')\n",
    "    # сделаем графики красивыми:\n",
    "    plt.axis('equal')\n",
    "    limits = [np.minimum(np.amin(data[:,0]), np.amin(data[:,1]))-0.5,\n",
    "              np.maximum(np.amax(data[:,0]), np.amax(data[:,1]))+0.5]\n",
    "    plt.xlim(limits[0],limits[1])\n",
    "    plt.ylim(limits[0],limits[1])\n",
    "    plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA(n_components=2)\n",
    "model.fit(data)\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "# построим истинные компоненты, вдоль которых максимальна дисперсия данных\n",
    "plt.plot(data[:,0], (W_true[0,0]/W_true[0,1])*data[:,0], color=\"g\")\n",
    "plt.plot(data[:,0], (W_true[1,0]/W_true[1,1])*data[:,0], color=\"g\")\n",
    "# построим компоненты, полученные с использованием метода PCA:\n",
    "plot_principal_components(data, model, scatter=False, legend=False)\n",
    "c_patch = mpatches.Patch(color='c', label='Principal components')\n",
    "plt.legend(handles=[g_patch, c_patch])\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что уже при небольшом количестве данных они отличаются незначительно. Увеличим размер выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_large = np.random.multivariate_normal(mu, C, size=5000)\n",
    "\n",
    "model = PCA(n_components=2)\n",
    "model.fit(data_large)\n",
    "plt.scatter(data_large[:,0], data_large[:,1], alpha=0.1)\n",
    "# построим истинные компоненты, вдоль которых максимальна дисперсия данных\n",
    "plt.plot(data_large[:,0], (W_true[0,0]/W_true[0,1])*data_large[:,0], color=\"g\")\n",
    "plt.plot(data_large[:,0], (W_true[1,0]/W_true[1,1])*data_large[:,0], color=\"g\")\n",
    "# построим компоненты, полученные с использованием метода PCA:\n",
    "plot_principal_components(data_large, model, scatter=False, legend=False)\n",
    "c_patch = mpatches.Patch(color='c', label='Principal components')\n",
    "plt.legend(handles=[g_patch, c_patch])\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом случае главные компоненты значительно точнее приближают истинные направления данных, вдоль которых наблюдается наибольшая дисперсия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Статистический взгляд на модель\n",
    "\n",
    "Как формализовать предположения метода, указанные выше? При помощи вероятностной модели!\n",
    "\n",
    "Задача, стоящая за любым методом уменьшения размерности: получить из набора зашумлённых признаков $X$ истинные значения $Y$, которые на самом деле определяют набор данных (т.е. сведение датасета с большим количеством признаков к данным, имеющим т.н. \"эффективную размерность\").\n",
    "\n",
    "В случае метода главных компонент мы хотим найти направления, вдоль которых максимальна дисперсия, с учётом описанных выше предположений о структуре данных и компонент.\n",
    "\n",
    "Материал, описанный ниже в данной секции, не обязателен для ознакомления для выполнения следующего задания, т.к. требует некоторых знаний статистики.\n",
    "\n",
    "Для тех, кто собирается его пропустить: в конце раздела мы получим метрику качества, которая должна определять, насколько данные хорошо описываются построенной моделью при заданном числе компонент. Отбор признаков при этом сводится к тому, что мы выбираем то количество компонент, при котором используемая метрика (логарифм правдоподобия) является максимальной.\n",
    "\n",
    "С учётом предположений задача метода главных компонент выглядит следующим образом:\n",
    "\n",
    "$$ x = Wy + \\mu + \\epsilon$$\n",
    "\n",
    "где:\n",
    "- $x$ -- наблюдаемые данные\n",
    "- $W$ -- матрица главных компонент (каждый стобец -- одна компонента)\n",
    "- $y$ -- их проекция на главные компоненты\n",
    "- $\\mu$ -- среднее наблюдаемых данных\n",
    "- $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2I)$ -- нормальный шум\n",
    "\n",
    "Исходя из распределения шума, выпишем распределение на $x$:\n",
    "\n",
    "$$p(x \\mid y) = \\mathcal{N}(Wx + \\mu, \\sigma^2I) $$\n",
    "\n",
    "Введём априорное распределение на $y$:\n",
    "\n",
    "$$p(y) = \\mathcal{N}(0, 1)$$\n",
    "\n",
    "Выведем из этого при помощи формулы Байеса маргинальное распределение на $p(x)$:\n",
    "\n",
    "$$p(x) = \\mathcal{N}(\\mu, \\sigma^2I + WW^T)$$\n",
    "\n",
    "Тогда правдоподобие набора данных при условии используемой модели выглядит следующим образом:\n",
    "\n",
    "$$\\mathcal{L} = \\sum_{i=1}^N \\log p(x_i) = -N/2 \\Big( d\\log(2\\pi) + \\log |C| + \\text{tr}(C^{-1}S) \\Big)$$\n",
    "\n",
    "где:\n",
    "- $C = \\sigma^2I + WW^T$ -- матрица ковариации в маргинальной модели\n",
    "- $S = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)(x_i - \\mu)^T$ -- выборочная ковариация\n",
    "\n",
    "Значение $\\mathcal{L}$ имеет смысл логарифма вероятности получения набора данных $X$ при условии, что он удовлетворяет предположениям модели метода главных компонент. Чем оно больше -- тем лучше модель описывает наблюдаемые данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Автоматическое уменьшение размерности данных при помощи логарифма правдоподобия $\\mathcal{L}$\n",
    "\n",
    "Рассмотрим набор данных размерности $D$, чья реальная размерность значительно меньше наблюдаемой (назовём её $d$). От вас требуется:\n",
    "\n",
    "1. Для каждого значения $\\hat{d}$ в интервале [1,D] построить модель PCA с $\\hat{d}$ главными компонентами.\n",
    "2. Оценить средний логарифм правдоподобия данных для каждой модели на генеральной совокупности, используя метод кросс-валидации с 3 фолдами (итоговая оценка значения логарифма правдоподобия усредняется по всем фолдам).\n",
    "3. Найти модель, для которой он максимален, и внести в файл ответа число компонент в данной модели, т.е. значение $\\hat{d}_{opt}$.\n",
    "\n",
    "Для оценки логарифма правдоподобия модели для заданного числа главных компонент при помощи метода кросс-валидации используйте следующие функции:\n",
    "\n",
    "    model = PCA(n_components=n)\n",
    "    scores = cv_score(model, data)\n",
    "        \n",
    "Обратите внимание, что scores -- это вектор, длина которого равна числу фолдов. Для получения оценки на правдоподобие модели его значения требуется усреднить.\n",
    "\n",
    "Для визуализации оценок можете использовать следующую функцию:\n",
    "\n",
    "    plot_scores(d_scores)\n",
    "    \n",
    "которой на вход передаётся вектор полученных оценок логарифма правдоподобия данных для каждого $\\hat{d}$.\n",
    "\n",
    "Для интересующихся: данные для заданий 1 и 2 были сгенерированны в соответствии с предполагаемой PCA моделью. То есть: данные $Y$ с эффективной размерностью $d$, полученные из независимых равномерных распределений, линейно траснформированны случайной матрицей $W$ в пространство размерностью $D$, после чего ко всем признакам был добавлен независимый нормальный шум с дисперсией $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score as cv_score\n",
    "\n",
    "def plot_scores(d_scores):\n",
    "    n_components = np.arange(1,d_scores.size+1)\n",
    "    plt.plot(n_components, d_scores, 'b', label='PCA scores')\n",
    "    plt.xlim(n_components[0], n_components[-1])\n",
    "    plt.xlabel('n components')\n",
    "    plt.ylabel('cv scores')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "def write_answer_1(optimal_d):\n",
    "    with open(\"pca_answer1.txt\", \"w\") as fout:\n",
    "        fout.write(str(optimal_d))\n",
    "        \n",
    "data = pd.read_csv('data_task1.csv')\n",
    "\n",
    "# place your code here\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вариационный взгляд на модель\n",
    "\n",
    "Мы знаем, что каждой главной компоненте соответствует описываемая ей дисперсия данных (дисперсия данных при проекции на эту компоненту). Она численно равна значению диагональных элементов матрицы $\\Lambda$, получаемой из спектрального разложения матрицы ковариации данных (смотри теорию выше).\n",
    "\n",
    "Исходя из этого, мы можем отсортировать дисперсию данных вдоль этих компонент по убыванию, и уменьшить размерность данных, отбросив $q$ итоговых главных компонент, имеющих наименьшую дисперсию.\n",
    "\n",
    "Делать это можно двумя разными способами. Например, если вы вдальнейшем обучаете на данных с уменьшенной размерностью модель классификации или регрессии, то можно запустить итерационный процесс: удалять компоненты с наименьшей дисперсией по одной, пока качество итоговой модели не станет значительно хуже.\n",
    "\n",
    "Более общий способ отбора признаков заключается в том, что вы можете посмотреть на разности в дисперсиях в отсортированном ряде $\\lambda_{(1)} > \\lambda_{(2)} > \\dots > \\lambda_{(D)}$: $\\lambda_{(1)}-\\lambda_{(2)}, \\dots, \\lambda_{(D-1)} - \\lambda_{(D)}$, и удалить те компоненты, на которых разность будет наибольшей. Именно этим методом вам и предлагается воспользоваться для тестового набора данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2. Ручное уменьшение размерности признаков посредством анализа дисперсии данных вдоль главных компонент\n",
    "\n",
    "Рассмотрим ещё один набор данных размерности $D$, чья реальная размерность значительно меньше наблюдаемой (назовём её также $d$). От вас требуется:\n",
    "\n",
    "1. Построить модель PCA с $D$ главными компонентами по этим данным.\n",
    "2. Спроецировать данные на главные компоненты.\n",
    "3. Оценить их дисперсию вдоль главных компонент.\n",
    "4. Отсортировать дисперсии в порядке убывания и получить их попарные разности: $\\lambda_{(i-1)} - \\lambda_{(i)}$.\n",
    "5. Найти разность с наибольшим значением и получить по ней оценку на эффективную размерность данных $\\hat{d}$.\n",
    "6. Построить график дисперсий и убедиться, что полученная оценка на $\\hat{d}_{opt}$ действительно имеет смысл, после этого внести полученное значение $\\hat{d}_{opt}$ в файл ответа.\n",
    "\n",
    "Для построения модели PCA используйте функцию:\n",
    "\n",
    "    model.fit(data)\n",
    "    \n",
    "Для трансформации данных используйте метод:\n",
    "\n",
    "    model.transform(data)\n",
    "    \n",
    "Оценку дисперсий на трансформированных данных от вас потребуется реализовать вручную. Для построения графиков можно воспользоваться функцией\n",
    "\n",
    "    plot_variances(d_variances)\n",
    "    \n",
    "которой следует передать на вход отсортированный по убыванию вектор дисперсий вдоль компонент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score as cv_score\n",
    "\n",
    "def plot_variances(d_variances):\n",
    "    n_components = np.arange(1,d_variances.size+1)\n",
    "    plt.plot(n_components, d_variances, 'b', label='Component variances')\n",
    "    plt.xlim(n_components[0], n_components[-1])\n",
    "    plt.xlabel('n components')\n",
    "    plt.ylabel('variance')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "def write_answer_2(optimal_d):\n",
    "    with open(\"pca_answer2.txt\", \"w\") as fout:\n",
    "        fout.write(str(optimal_d))\n",
    "        \n",
    "data = pd.read_csv('data_task2.csv')\n",
    "\n",
    "# place your code here\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Интерпретация главных компонент\n",
    "\n",
    "В качестве главных компонент мы получаем линейные комбинации исходных призанков, поэтому резонно возникает вопрос об их интерпретации.\n",
    "\n",
    "Для этого существует несколько подходов, мы рассмотрим два:\n",
    "- рассчитать взаимосвязи главных компонент с исходными признаками\n",
    "- рассчитать вклады каждого конкретного наблюдения в главные компоненты\n",
    "\n",
    "Первый способ подходит в том случае, когда все объекты из набора данных не несут для нас никакой семантической информации, которая уже не запечатлена в наборе признаков.\n",
    "\n",
    "Второй способ подходит для случая, когда данные имеют более сложную структуру. Например, лица для человека несут больший семантический смысл, чем вектор значений пикселей, которые анализирует PCA.\n",
    "\n",
    "Рассмотрим подробнее способ 1: он заключается в подсчёте коэффициентов корреляций между исходными признаками и набором главных компонент.\n",
    "\n",
    "Так как метод главных компонент является линейным, то предлагается для анализа использовать корреляцию Пирсона, выборочный аналог которой имеет следующую формулу:\n",
    "\n",
    "$$r_{jk} = \\frac{\\sum_{i=1}^N (x_{ij} - \\bar{x}_j) (y_{ik} - \\bar{y}_k)}{\\sqrt{\\sum_{i=1}^N (x_{ij} - \\bar{x}_j)^2 \\sum_{i=1}^N (y_{ik} - \\bar{y}_k)^2}} $$\n",
    "\n",
    "где:\n",
    "- $\\bar{x}_j$ -- среднее значение j-го признака,\n",
    "- $\\bar{y}_k$ -- среднее значение проекции на k-ю главную компоненту.\n",
    "\n",
    "Корреляция Пирсона является мерой линейной зависимости. Она равна 0 в случае, когда величины независимы, и $\\pm 1$, если они линейно зависимы. Исходя из степени корреляции новой компоненты с исходными признаками, можно строить её семантическую интерпретацию, т.к. смысл исходных признаков мы знаем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3. Анализ главных компонент при помощи корреляций с исходными признаками.\n",
    "\n",
    "1. Обучите метод главных компонент на датасете iris, получите преобразованные данные.\n",
    "2. Посчитайте корреляции исходных признаков с их проекциями на первые две главные компоненты.\n",
    "3. Для каждого признака найдите компоненту (из двух построенных), с которой он коррелирует больше всего.\n",
    "4. На основании п.3 сгруппируйте признаки по компонентам. Составьте два списка: список номеров признаков, которые сильнее коррелируют с первой компонентой, и такой же список для второй. Нумерацию начинать с единицы. Передайте оба списка функции write_answer_3.\n",
    "\n",
    "Набор данных состоит из 4 признаков, посчитанных для 150 ирисов. Каждый из них принадлежит одному из трёх видов. Визуализацию проекции данного датасета на две компоненты, которые описывают наибольшую дисперсию данных, можно получить при помощи функции\n",
    "\n",
    "    plot_iris(transformed_data, target, target_names)\n",
    "    \n",
    "на вход которой требуется передать данные, преобразованные при помощи PCA, а также информацию о классах. Цвет точек отвечает одному из трёх видов ириса.\n",
    "\n",
    "Для того чтобы получить имена исходных признаков, используйте следующий список:\n",
    "\n",
    "    iris.feature_names\n",
    "    \n",
    "При подсчёте корреляций не забудьте центрировать признаки и проекции на главные компоненты (вычитать из них среднее)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "def plot_iris(transformed_data, target, target_names):\n",
    "    plt.figure()\n",
    "    for c, i, target_name in zip(\"rgb\", [0, 1, 2], target_names):\n",
    "        plt.scatter(transformed_data[target == i, 0],\n",
    "                    transformed_data[target == i, 1], c=c, label=target_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def write_answer_3(list_pc1, list_pc2):\n",
    "    with open(\"pca_answer3.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(num) for num in list_pc1]))\n",
    "        fout.write(\" \")\n",
    "        fout.write(\" \".join([str(num) for num in list_pc2]))\n",
    "\n",
    "# загрузим датасет iris\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "# place your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Интерпретация главных компонент с использованием данных\n",
    "\n",
    "Рассмотрим теперь величину, которую можно проинтерпретировать, как квадрат косинуса угла между объектом выборки и главной компонентой:\n",
    "\n",
    "$$ cos^2_{ik} = \\frac{f_{ik}^2}{\\sum_{\\ell=1}^d f_{i\\ell}^2} $$\n",
    "\n",
    "где\n",
    "- i -- номер объекта\n",
    "- k -- номер главной компоненты\n",
    "- $f_{ik}$ -- модуль центрированной проекции объекта на компоненту\n",
    "\n",
    "Очевидно, что\n",
    "\n",
    "$$ \\sum_{k=1}^d cos^2_{ik} = 1 $$\n",
    "\n",
    "Это значит, что для каждого объекта мы в виде данной величины получили веса, пропорциональные вкладу, которую вносит данный объект в дисперсию каждой компоненты. Чем больше вклад, тем более значим объект для описания конкретной главной компоненты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4. Анализ главных компонент при помощи вкладов в их дисперсию отдельных объектов\n",
    "\n",
    "1. Загрузите датасет лиц Olivetti Faces и обучите на нём модель RandomizedPCA (используется при большом количестве признаков и работает быстрее, чем обычный PCA). Получите проекции признаков на 10 первых главных компонент.\n",
    "2. Посчитайте для каждого объекта его относительный вклад в дисперсию каждой из 10 компонент, используя формулу из предыдущего раздела (d = 10).\n",
    "3. Для каждой компоненты найдите и визуализируйте лицо, которое вносит наибольший относительный вклад в неё. Для визуализации используйте функцию\n",
    "\n",
    "        plt.imshow(image.reshape(image_shape))\n",
    "        \n",
    "4. Передайте в функцию write_answer_4 список номеров лиц с наибольшим относительным вкладом в дисперсию каждой из компонент, список начинается с 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def write_answer_4(list_pc):\n",
    "    with open(\"pca_answer4.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(num) for num in list_pc]))\n",
    "\n",
    "data = fetch_olivetti_faces(shuffle=True, random_state=0).data\n",
    "image_shape = (64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ основных недостатков метода главных компонент\n",
    "\n",
    "Рассмотренные выше задачи являются, безусловно, модельными, потому что данные для них были сгенерированы в соответствии с предположениями метода главных компонент. На практике эти предположения, естественно, выполняются далеко не всегда. Рассмотрим типичные ошибки PCA, которые следует иметь в виду перед тем, как его применять."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Направления с максимальной дисперсией в данных неортогональны\n",
    "\n",
    "Рассмотрим случай выборки, которая сгенерирована из двух вытянутых нормальных распределений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = np.array([[10,0],[0,0.5]])\n",
    "phi = np.pi/3\n",
    "C2 = np.dot(C1, np.array([[np.cos(phi), np.sin(phi)],\n",
    "                          [-np.sin(phi),np.cos(phi)]]))\n",
    "\n",
    "data = np.vstack([np.random.multivariate_normal(mu, C1, size=50),\n",
    "                  np.random.multivariate_normal(mu, C2, size=50)])\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "# построим истинные интересующие нас компоненты\n",
    "plt.plot(data[:,0], np.zeros(data[:,0].size), color=\"g\")\n",
    "plt.plot(data[:,0], 3**0.5*data[:,0], color=\"g\")\n",
    "# обучим модель pca и построим главные компоненты\n",
    "model = PCA(n_components=2)\n",
    "model.fit(data)\n",
    "plot_principal_components(data, model, scatter=False, legend=False)\n",
    "c_patch = mpatches.Patch(color='c', label='Principal components')\n",
    "plt.legend(handles=[g_patch, c_patch])\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В чём проблема, почему pca здесь работает плохо? Ответ прост: интересующие нас компоненты в данных коррелированны между собой (или неортогональны, в зависимости от того, какой терминологией пользоваться). Для поиска подобных преобразований требуются более сложные методы, которые уже выходят за рамки метода главных компонент.\n",
    "\n",
    "Для интересующихся: то, что можно применить непосредственно к выходу метода главных компонент, для получения подобных неортогональных преобразований, называется методами ротации. Почитать о них можно в связи с другим методом уменьшения размерности, который называется Factor Analysis (FA), но ничего не мешает их применять и к главным компонентам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интересное направление в данных не совпадает с направлением максимальной дисперсии\n",
    "\n",
    "Рассмотрим пример, когда дисперсии не отражают интересующих нас направлений в данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([[0.5,0],[0,10]])\n",
    "mu1 = np.array([-2,0])\n",
    "mu2 = np.array([2,0])\n",
    "\n",
    "data = np.vstack([np.random.multivariate_normal(mu1, C, size=50),\n",
    "                  np.random.multivariate_normal(mu2, C, size=50)])\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "# обучим модель pca и построим главные компоненты\n",
    "model = PCA(n_components=2)\n",
    "model.fit(data)\n",
    "plot_principal_components(data, model)\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что в данном случае метод главных компонент будет считать вертикальную компоненту более значимой для описания набора данных, чем горизонтальную. \n",
    "\n",
    "Но, например, в случае, когда данные из левого и правого кластера относятся к разным классам, для их линейной разделимости вертикальная компонента является шумовой. Несмотря на это, её метод главных компонент никогда шумовой не признает, и есть вероятность, что отбор признаков с его помощью выкинет из ваших данных значимые для решаемой вами задачи компоненты просто потому, что вдоль них значения имеют низкую дисперсию.\n",
    "\n",
    "Справляться с такими ситуациями могут некоторые другие методы уменьшения размерности данных, например, метод независимых компонент (Independent Component Analysis, ICA)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
